Advanced techniques for through and contextually
Interpreting Noun-Noun Compounds
Abstract
Thisstudyexaminestheeffectivenessoftransferlearningandmulti-tasklearning
inthecontextofacomplexsemanticclassificationproblem: understandingthe
meaningofnoun-nouncompounds. Throughaseriesofdetailedexperimentsand
anin-depthanalysisoferrors,wedemonstratethatemployingtransferlearningby
initializingparametersandmulti-tasklearningthroughparametersharingenablesa
neuralclassificationmodeltobettergeneralizeacrossadatasetcharacterizedbya
highlyunevendistributionofsemanticrelationships. Furthermore,weillustrate
howutilizingdualannotations,whichinvolvetwodistinctsetsofrelationsapplied
tothesamecompounds,canenhancetheoverallprecisionofaneuralclassifierand
improveitsF1scoresforlesscommonyetmorechallengingsemanticrelations.
1 Introduction
Noun-noun compound interpretation involves determining the semantic connection between two
nouns(ornounphrasesinmulti-wordcompounds). Forinstance,inthecompound"streetprotest,"
thetaskistoidentifythesemanticrelationshipbetween"street"and"protest,"whichisalocative
relationinthisexample. Giventheprevalenceofnoun-nouncompoundsinnaturallanguageandits
significancetoothernaturallanguageprocessing(NLP)taskslikequestionansweringandinformation
retrieval,understandingnoun-nouncompoundshasbeenextensivelystudiedintheoreticallinguistics,
psycholinguistics,andcomputationallinguistics.
Incomputationallinguistics,noun-nouncompoundinterpretationistypicallytreatedasanautomatic
classification task. Various machine learning (ML) algorithms and models, such as Maximum
Entropy, Support Vector Machines, and Neural Networks, have been employed to decipher the
semanticsofnominalcompounds. Thesemodelsutilizeinformationfromlexicalsemantics,like
WordNet-basedfeatures,anddistributionalsemantics,suchaswordembeddings. However,noun-
noun compound interpretation remains a challenging NLP problem due to the high productivity
ofnoun-nouncompoundingasalinguisticstructureandthedifficultyinderivingthesemanticsof
noun-nouncompoundsfromtheirconstituents. OurresearchcontributestoadvancingNLPresearch
onnoun-nouncompoundinterpretationthroughtheapplicationoftransferandmulti-tasklearning.
Theapplicationoftransferlearning(TL)andmulti-tasklearning(MTL)inNLPhasgainedsignificant
attentioninrecentyears,yieldingvaryingoutcomesbasedonthespecifictasks,modelarchitectures,
anddatasetsinvolved. Thesevaryingresults,combinedwiththefactthatneitherTLnorMTLhas
beenpreviouslyappliedtonoun-nouncompoundinterpretation,motivateourthoroughempirical
investigationintotheuseofTLandMTLforthistask. Ouraimisnotonlytoaddtotheexisting
researchontheeffectivenessofTLandMTLforsemanticNLPtasksgenerallybutalsotoascertain
theirspecificadvantagesforcompoundinterpretation.
A key reason for utilizing multi-task learning is to enhance generalization by making use of the
domain-specificdetailspresentinthetrainingdataofrelatedtasks. Inthisstudy,wedemonstratethat
TLandMTLcanserveasaformofregularization,enablingthepredictionofinfrequentrelations
within a dataset marked by a highly skewed distribution of relations. This dataset is particularly
well-suitedforTLandMTLexperimentation,aselaboratedinSection3.Ourcontributionsaresummarizedasfollows:
1. Throughmeticulousanalysisofresults,wediscoverthatTLandMTL,especiallywhenapplied
to the embedding layer, enhance overall accuracy and F1 scores for less frequent relations in a
highlyskeweddataset,comparedtoarobustsingle-tasklearningbaseline. 2. Althoughourresearch
concentratesonTLandMTL,wepresent,toourknowledge,thefirstexperimentalresultsonthe
relativelyrecentdatasetfromFares(2016).
2 RelatedWork
Approachestointerpretingnoun-nouncompoundsdifferbasedontheclassificationofcompound
relations,aswellasthemachinelearningmodelsandfeaturesemployedtolearntheserelations. For
instance,somedefineabroadsetofrelations,whileothersemployamoredetailedclassification.
Someresearcherschallengetheideathatnoun-nouncompoundscanbeinterpretedusingafixed,
predetermined set of relations, proposing alternative methods based on paraphrasing. We center
ourattentiononmethodsthatframetheinterpretationproblemasaclassificationtaskinvolvinga
fixed,predeterminedsetofrelations. Variousmachinelearningmodelshavebeenappliedtothis
task,includingnearestneighborclassifiersthatusesemanticsimilaritybasedonlexicalresources,
kernel-based methods like SVMs that utilize lexical and relational features, Maximum Entropy
modelsthatincorporateawiderangeoflexicalandsurfaceformfeatures,andneuralnetworksthat
relyonwordembeddingsorcombinewordembeddingswithpathembeddings. Amongthesestudies,
some have utilized the same dataset. To our knowledge, TL and MTL have not been previously
appliedtocompoundinterpretation. Therefore,wereviewpriorresearchonTLandMTLinother
NLPtasks.
SeveralrecentstudieshaveconductedextensiveexperimentsontheapplicationofTLandMTLtoa
varietyofNLPtasks,suchasnamedentityrecognition,semanticlabeling,sentence-levelsentiment
classification,super-tagging,chunking,andsemanticdependencyparsing. Theconsensusamong
thesestudiesisthattheadvantagesofTLandMTLarelargelycontingentonthecharacteristicsofthe
tasksinvolved,includingtheunevennessofthedatadistribution,thesemanticrelatednessbetween
thesourceandtargettasks,thelearningtrajectoryoftheauxiliaryandmaintasks(wheretargettasks
thatquicklyreachaplateaubenefitmostfromnon-plateauingauxiliarytasks), andthestructural
similaritybetweenthetasks. BesidesdifferingintheNLPtaskstheyinvestigate,theaforementioned
studiesemployslightlyvarieddefinitionsofTLandMTL.Ourresearchalignswithcertainstudiesin
thatweapplyTLandMTLtolearndifferentsemanticannotationsofnoun-nouncompoundsusing
thesamedataset. However,ourexperimentaldesignismoreakintootherworkinthatweexperiment
withinitializingparametersacrossalllayersoftheneuralnetworkandconcurrentlytrainasingle
MTLmodelontwosetsofrelations.
3 TaskDefinitionandDataset
Theobjectiveofthistaskistotrainamodeltocategorizethesemanticrelationshipsbetweenpairs
ofnounsinalabeleddataset,whereeachpairformsanoun-nouncompound. Thecomplexityof
thistaskisinfluencedbyfactorssuchasthelabelsetusedanditsdistribution. Fortheexperiments
detailedinthispaper,weutilizeanoun-nouncompoundsdatasetthatfeaturescompoundsannotated
withtwodistincttaxonomiesofrelations. Thismeansthateachnoun-nouncompoundisassociated
withtwodifferentrelations,eachbasedondifferentlinguistictheories. Thisdatasetisderivedfrom
establishedlinguisticresources, includingNomBankandthePragueCzech-EnglishDependency
Treebank2.0(PCEDT).Wechosethisdatasetfortwoprimaryreasons: firstly,thedualannotationof
relationsonthesamesetofcompoundsisidealforexploringTLandMTLapproaches;secondly,
aligningtwodifferentannotationframeworksonthesamedataallowsforacomparativeanalysis
acrosstheseframeworks.
Specifically,weuseaportionofthedataset,focusingontype-basedinstancesoftwo-wordcompounds.
The original dataset also encompasses multi-word compounds (those made up of more than two
nouns)andmultipleinstancespercompoundtype. Wefurtherdividethedatasetintothreeparts:
training,development,andtestsets.Table1detailsthenumberofcompoundtypesandthevocabulary
sizeforeachset,includingabreakdownofwordsappearingintheright-most(rightconstituents)
andleft-most(leftconstituents)positions. Thetwolabelsetsconsistof35PCEDTfunctorsand18
2NomBankargumentandadjunctrelations. AsdiscussedinSection7.1,theselabelsetshaveahighly
unevendistribution.
Table1: Characteristicsofthenoun-nouncompounddatasetusedinourexperiments. Thenumbers
inthistablecorrespondtoasubsetofthedataset,seeSection3.
Train Dev Test
Compounds 6932 920 1759
Vocabsize 4102 1163 1772
Rightconstituents 2304 624 969
Leftconstituents 2405 618 985
ManyrelationsinPCEDTandNomBankconceptuallydescribesimilarsemanticideas,astheyare
usedtoannotatethesemanticsofthesametext. Forinstance,thetemporalandlocativerelationsin
NomBank(ARGM-TMPandARGM-LOC,respectively)andtheirPCEDTcounterparts(TWHEN
andLOC)exhibitrelativelyconsistentbehavioracrossframeworks,astheyannotatemanyofthe
samecompounds. However,somerelationsthataretheoreticallysimilardonotalignwellinpractice.
For example, the functor AIM in PCEDT and the modifier argument ARGM-PNC in NomBank
expressasomewhatrelatedsemanticconcept(purpose),butthereisminimaloverlapbetweenthe
setsofcompoundstheyannotate. Nevertheless,itisreasonabletoassumethatthesemanticsimilarity
inthelabelsets,whereitexists,canbeleveragedthroughtransferandmulti-tasklearning,especially
sincetheoveralldistributionofrelationsdiffersbetweenthetwoframeworks.
4 Transfervs. Multi-TaskLearning
Inthissection,weemploytheterminologyanddefinitionsestablishedbyPanandYang(2010)to
articulateourframeworkfortransferandmulti-tasklearning. Ourclassificationtaskcanbedescribed
intermsofalltrainingpairs(X,Y)andaprobabilitydistributionP(X),whereXrepresentstheinput
featurespace,Ydenotesthesetofalllabels,andNisthetrainingdatasize. Thedomainofataskis
definedbyX,P(X).Ourgoalistolearnafunctionf(X)thatpredictsYbasedontheinputfeaturesX.
ConsideringtwoMLtasks,TaandTb,wewouldtraintwodistinctmodelstolearnseparatefunctions
faandfbforpredictingYaandYbinasingle-tasklearningscenario. However, ifTaandTbare
related,eitherexplicitlyorimplicitly,TLandMTLcanenhancethegeneralizationofeitherorboth
tasks. Twotasksaredeemedrelatedwhentheirdomainsaresimilarbuttheirlabelsetsdiffer,orwhen
theirdomainsaredissimilarbuttheirlabelsetsareidentical. Consequently,noun-nouncompound
interpretationusingthedatasetiswell-suitedforTLandMTL,asthetrainingexamplesareidentical,
butthelabelsetsaredistinct.
Forclarity,wedifferentiatebetweentransferlearningandmulti-tasklearninginthispaper,despite
thesetermssometimesbeingusedinterchangeablyintheliterature. WedefineTLastheutilizationof
parametersfromamodeltrainedonTatoinitializeanothermodelforTb. Incontrast,MTLinvolves
trainingpartsofthesamemodeltolearnbothTaandTb,essentiallylearningonesetofparameters
forbothtasks. Theconceptistotrainasinglemodelsimultaneouslyonbothtasks,whereonetask
introducesaninductivebiasthataidsthemodelingeneralizingoverthemaintask. Itisimportantto
notethatthisdoesnotnecessarilyimplythatweaimtouseasinglemodeltopredictbothlabelsets
inpractice.
5 NeuralClassificationModels
Thissectionintroducestheneuralclassificationmodelsutilizedinourexperiments. Todiscernthe
impactofTLandMTL,weinitiallypresentasingle-tasklearningmodel,whichactsasourbaseline.
Subsequently,weemploythissamemodeltoimplementTLandMTL.
5.1 Single-TaskLearningModel
Inoursingle-tasklearning(STL)configuration,wetrainandfine-tuneafeed-forwardneuralnetwork
inspiredbytheneuralclassifierproposedbyDimaandHinrichs(2015). Thisnetworkcomprisesfour
layers: 1)aninputlayer,2)anembeddinglayer,3)ahiddenlayer,and4)anoutputlayer. Theinput
3layerconsistsoftwointegersthatindicatetheindicesofacompound’sconstituentsintheembedding
layer,wherethewordembeddingvectorsarestored. Theseselectedvectorsarethenpassedtoafully
connectedhiddenlayer,thesizeofwhichmatchesthedimensionalityofthewordembeddingvectors.
Finally,asoftmaxfunctionisappliedtotheoutputlayertoselectthemostprobablerelation.
Thecompound’sconstituentsarerepresentedusinga300-dimensionalwordembeddingmodeltrained
onanEnglishWikipediadumpandtheEnglishGigawordFifthEdition. Theembeddingmodelwas
trainedbyFaresetal. (2017). Ifawordisnotfoundduringlookupintheembeddingmodel,we
checkifthewordisuppercasedandattempttofindthelowercaseversion. Forhyphenatedwords
notfoundintheembeddingvocabulary,wesplitthewordatthehyphenandaveragethevectorsof
itsparts,iftheyarepresentinthevocabulary. Ifthewordremainsunrepresentedafterthesesteps,a
designatedvectorforunknownwordsisemployed.
5.1.1 ArchitectureandHyperparameters
Ourselectionofhyperparametersisinformedbymultipleroundsofexperimentationwiththesingle-
tasklearningmodel,aswellasthechoicesmadebypriorwork. Theweightsoftheembeddinglayer
areupdatedduringthetrainingofallmodels. WeutilizetheAdaptiveMomentEstimation(Adam)
optimizationfunctionacrossallmodels,withalearningratesetto0.001. Thelossfunctionemployed
isthenegative-loglikelihood. ASigmoidactivationfunctionisusedfortheunitsinthehiddenlayer.
Allmodelsaretrainedwithmini-batchesofsizefive. Themaximumnumberofepochsiscapped
at50,butanearlystoppingcriterionbasedonthemodel’saccuracyonthevalidationsplitisalso
implemented. Thismeansthattrainingishaltedifthevalidationaccuracydoesnotimproveoverfive
consecutiveepochs. AllmodelsareimplementedinKeras,usingTensorFlowasthebackend. TheTL
andMTLmodelsaretrainedusingthesamehyperparametersastheSTLmodel.
5.2 TransferLearningModels
Inourexperiments,transferlearninginvolvestraininganSTLmodelonPCEDTrelationsandthen
using some of its weights to initialize another model for NomBank relations. Given the neural
classifier architecture detailed in Section 5.1, we identify three ways to implement TL: 1) TLE:
Transferringtheembeddinglayerweights,2)TLH:Transferringthehiddenlayerweights,and3)
TLEH:Transferringboththeembeddingandhiddenlayerweights. Furthermore,wedifferentiate
between transfer learning from PCEDT to NomBank and vice versa. This results in six setups,
as shown in Table 2. We do not apply TL (or MTL) to the output layer because it is task- or
dataset-specific.
5.3 Multi-TaskLearningModels
In MTL, we train a single model to simultaneously learn both PCEDT and NomBank relations,
meaningallMTLmodelshavetwoobjectivefunctionsandtwooutputlayers. Weimplementtwo
MTLsetups: MTLE,whichfeaturesasharedembeddinglayerbuttwotask-specifichiddenlayers,
andMTLF,whichhasnotask-specificlayersasidefromtheoutputlayer(i.e.,boththeembedding
andhiddenlayersareshared). Wedistinguishbetweentheauxiliaryandmaintasksbasedonwhich
validationaccuracy(NomBank’sorPCEDT’s)ismonitoredbytheearlystoppingcriterion. This
leadstoatotaloffourMTLmodels,asshowninTable3.
6 ExperimentalResults
Tables2and3displaytheaccuraciesofthevariousTLandMTLmodelsonthedevelopmentandtest
splitsforNomBankandPCEDT.ThetoprowinbothtablesindicatestheaccuracyoftheSTLmodel.
All models were trained solely on the training split. Several insights can be gleaned from these
tables. Firstly,theaccuracyoftheSTLmodelsdecreaseswhenevaluatedonthetestsplitforboth
NomBankandPCEDT.Secondly,allTLmodelsachieveimprovedaccuracyontheNomBanktest
split,althoughtransferlearningdoesnotsignificantlyenhanceaccuracyonthedevelopmentsplitof
thesamedataset. TheMTLmodels,especiallyMTLF,haveadetrimentaleffectonthedevelopment
accuracyofNomBank,yetweobserveasimilarimprovement,aswithTL,onthetestsplit. Thirdly,
bothTLandMTLmodelsdemonstratelessconsistenteffectsonPCEDT(onbothdevelopmentand
testsplits)comparedtoNomBank. Forinstance,allTLmodelsyieldanabsoluteimprovementof
4about1.25pointsinaccuracyonNomBank,whereasinPCEDT,TLEclearlyoutperformstheother
twoTLmodels(TLEimprovesovertheSTLaccuracyby1.37points).
Table2: Accuracy(%)ofthetransferlearningmodels.
Model NomBank PCEDT
Dev Test Dev Test
STL 78.15 76.75 58.80 56.05
TLE 78.37 78.05 59.57 57.42
TLH 78.15 78.00 59.24 56.51
TLEH 78.48 78.00 59.89 56.68
Table3: Accuracy(%)oftheMTLmodels.
Model NomBank PCEDT
Dev Test Dev Test
STL 78.15 76.75 58.80 56.05
MTLE 77.93 78.45 59.89 56.96
MTLF 76.74 78.51 58.91 56.00
Overall,theSTLmodels’accuracydeclineswhentestedontheNomBankandPCEDTtestsplits,
comparedtotheirperformanceonthedevelopmentsplit. Thiscouldsuggestoverfitting,especially
sinceourstoppingcriterionselectsthemodelwiththebestperformanceonthedevelopmentsplit.
Conversely,TLandMTLenhanceaccuracyonthetestsplits,despiteusingthesamestoppingcriterion
asSTL.Weinterpretthisasanimprovementinthemodels’abilitytogeneralize. However,since
theseimprovementsarerelativelyminor,wefurtheranalyzetheresultstounderstandifandhowTL
andMTLarebeneficial.
7 ResultsAnalysis
Thissectionprovidesadetailedanalysisofthemodels’performance,drawingoninsightsfromthe
datasetandtheclassificationerrorsmadebythemodels. Thediscussioninthefollowingsectionsis
primarilybasedontheresultsfromthetestsplit,asitislargerthanthedevelopmentsplit.
7.1 RelationDistribution
Toillustratethecomplexityofthetask,wedepictthedistributionofthemostfrequentrelationsin
NomBankandPCEDTacrossthethreedatasplitsinFigure1. Notably,approximately71.18%ofthe
relationsintheNomBanktrainingsplitareoftypeARG1(prototypicalpatient),while52.20%ofthe
PCEDTrelationsareoftypeRSTR(anunderspecifiedadnominalmodifier). Suchahighlyskewed
distributionmakeslearningsomeoftheotherrelationsmorechallenging,ifnotimpossibleincertain
cases. Infact,outofthe15NomBankrelationsobservedinthetestsplit,fiveareneverpredicted
byanyoftheSTL,TL,orMTLmodels. Similarly,ofthe26PCEDTrelationsinthetestsplit,only
sixarepredicted. However,theunpredictedrelationsareextremelyrareinthetrainingsplit(e.g.,23
PCEDTfunctorsappearlessthan20times),makingitdoubtfulwhetheranyMLmodelcouldlearn
themunderanycircumstances.
Giventhisimbalanceddistribution,itisevidentthataccuracyaloneisinsufficienttodeterminethe
best-performingmodel. Therefore,inthesubsequentsection,wereportandanalyzetheF1scoresof
thepredictedNomBankandPCEDTrelationsacrossallSTL,TL,andMTLmodels.
7.2 Per-RelationF1Scores
Tables4and5presenttheper-relationF1scoresforNomBankandPCEDT,respectively. Weonly
includeresultsforrelationsthatareactuallypredictedbyatleastoneofthemodels.
5Table4: Per-labelF1scoreontheNomBanktestsplit.
A0 A1 A2 A3 LOC MNR TMP
Count 132 1282 153 75 25 25 27
STL 49.82 87.54 45.78 60.81 28.57 29.41 66.67
TLE 55.02 87.98 41.61 60.14 27.91 33.33 63.83
TLH 54.81 87.93 42.51 60.00 25.00 35.29 65.31
TLEH 53.62 87.95 42.70 61.11 29.27 33.33 65.22
MTLE 54.07 88.34 42.86 61.97 30.00 28.57 66.67
MTLF 53.09 88.41 38.14 62.69 00.00 00.00 52.17
Table5: Per-labelF1scoreonthePCEDTtestsplit.
ACT TWHEN APP PAT REG RSTR
Count 89 14 118 326 216 900
STL 43.90 42.11 22.78 42.83 20.51 68.81
TLE 49.37 70.97 27.67 41.60 30.77 69.67
TLH 53.99 62.07 25.00 43.01 26.09 68.99
TLEH 49.08 64.52 28.57 42.91 28.57 69.08
MTLE 54.09 66.67 24.05 42.03 27.21 69.31
MTLF 47.80 42.11 25.64 40.73 19.22 68.89
SeveralnoteworthypatternsemergefromTables4and5. Firstly,theMTLFmodelappearstobe
detrimentaltobothdatasets,leadingtosignificantlydegradedF1scoresforfourNomBankrelations,
includingthelocativemodifierARGM-LOCandthemannermodifierARGM-MNR(abbreviatedas
LOCandMNRinTable4),whichthemodelfailstopredictaltogether. Thissamemodelexhibits
the lowest F1 score compared to all other models for two PCEDT relations: REG (expressing a
circumstance)andPAT(patient). ConsideringthattheMTLFmodelachievesthehighestaccuracy
ontheNomBanktestsplit(asshowninTable3),itbecomesevenmoreapparentthatrelyingsolely
onaccuracyscoresisinadequateforevaluatingtheeffectivenessofTLandMTLforthistaskand
dataset.
Secondly,withtheexceptionoftheMTLFmodel,allTLandMTLmodelsconsistentlyimprove
theF1scoreforallPCEDTrelationsexceptPAT.Notably,theF1scoresfortherelationsTWHEN
andACTshowasubstantialincreasecomparedtootherPCEDTrelationswhenonlytheembedding
layer’sweightsareshared(MTLE)ortransferred(TLE).Thisoutcomecanbepartiallyunderstood
by examining the correspondence matrices between NomBank arguments and PCEDT functors,
presentedinTables7and6. ThesetablesillustratehowPCEDTfunctorsmaptoNomBankarguments
inthetrainingsplit(Table6)andviceversa(Table7). Table6revealsthat80%ofthecompounds
annotatedasTWHENinPCEDTwereannotatedasARGM-TMPinNomBank. Additionally,47%of
ACT(Actor)relationsmaptoARG0(Proto-Agent)inNomBank.Whilethismappingisnotasdistinct
asonemighthope,itisstillrelativelyhighwhencomparedtohowotherPCEDTrelationsmapto
ARG0. The correspondence matrices also demonstrate that the presumed theoretical similarities
betweenNomBankandPCEDTrelationsdonotalwaysholdinpractice. Nevertheless,evensuch
imperfect correspondences can provide a training signal that assists the TL and MTL models in
learningrelationslikeTWHENandACT.
Since the TLE model outperforms STL in predicting REG by ten absolute points, we examined
all REG compounds correctly classified by TLE but misclassified by STL. We found that STL
misclassified them as RSTR, indicating that TL from NomBank helps TLE recover from STL’s
overgeneralizationinRSTRprediction.
ThetwoNomBankrelationsthatreceivethehighestboostinF1score(aboutfiveabsolutepoints)
areARG0andARGM-MNR,buttheimprovementinthelattercorrespondstoonlyoneadditional
compound,whichmightbeachanceoccurrence. Overall,TLandMTLfromNomBanktoPCEDT
aremorehelpfulthanthereverse. OneexplanationisthatfivePCEDTrelations(includingthefour
mostfrequentones)maptoARG1inNomBankinmorethan60%ofcasesforeachrelation,asseen
inthefirstrowsofTables6and7. ThissuggeststhattheweightslearnedtopredictPCEDTrelations
6Table6: CorrespondencematrixbetweenPCEDTfunctorsandNomBankarguments. Slotswith’-’
indicatezero,0.00representsaverysmallnumberbutnotzero.
A1 A2 A0 A3 LOC TMP MNR
RSTR 0.70 0.11 0.06 0.06 0.02 0.01 0.02
PAT 0.90 0.05 0.01 0.02 0.01 - 0.00
REG 0.78 0.10 0.04 0.06 0.00 0.00 0.00
APP 0.62 0.21 0.13 0.02 0.01 0.00 -
ACT 0.47 0.03 0.47 0.01 0.01 - 0.01
AIM 0.65 0.12 0.07 0.06 0.01 - -
TWHEN 0.10 0.03 - - - 0.80 -
Count 3617 1312 777 499 273 116 59
Table7: CorrespondencematrixbetweenNomBankargumentsandPCEDTfunctors.
RSTR PAT REG APP ACT AIM TWHEN
A1 0.51 0.54 0.12 0.06 0.03 0.02 0.00
A2 0.47 0.09 0.11 0.14 0.01 0.02 0.00
A0 0.63 0.03 0.07 0.13 0.26 0.02 -
A3 0.66 0.08 0.13 0.03 0.01 0.02 -
LOC 0.36 0.07 0.02 0.05 0.03 0.01 -
TMP 0.78 - 0.01 0.01 - - 0.01
MNR 0.24 0.05 0.01 - 0.03 - -
Count 4932 715 495 358 119 103 79
offerlittletonoinductivebiasforNomBankrelations. Conversely,themappingfromNomBankto
PCEDTshowsthatalthoughmanyNomBankargumentsmaptoRSTRinPCEDT,thepercentages
arelower,makingthemappingmorediverseanddiscriminative,whichseemstoaidTLandMTL
modelsinlearninglessfrequentPCEDTrelations.
TounderstandwhythePCEDTfunctorAIMisneverpredicteddespitebeingmorefrequentthan
TWHEN,wefoundthatAIMisalmostalwaysmisclassifiedasRSTRbyallmodels. Furthermore,
AIMandRSTRhavethehighestlexicaloverlapinthetrainingsetamongallPCEDTrelationpairs:
78.35%ofleftconstituentsand73.26%ofrightconstituentsofcompoundsannotatedasAIMoccur
inothercompoundsannotatedasRSTR.Thisexplainsthemodels’inabilitytolearnAIMbutraises
questionsabouttheirabilitytolearnrelationalrepresentations,whichweexplorefurtherinSection
7.3.
Table8: Macro-averageF1scoreonthetestsplit.
Model NomBank PCEDT
STL 52.66 40.15
TLE 52.83 48.34
TLH 52.98 46.52
TLEH 53.31 47.12
MTLE 53.21 47.23
MTLF 42.07 40.73
Finally,todemonstratethebenefitsofTLandMTLforNomBankandPCEDT,wereporttheF1
macro-averagescoresinTable8. Thisisarguablytheappropriateevaluationmeasureforimbalanced
classificationproblems. Notethatrelationsnotpredictedbyanymodelareexcludedfromthemacro-
averagecalculation. Table8clearlyshowsthatTLandMTLontheembeddinglayeryieldsignificant
improvementsforPCEDT,withabouta7-8pointincreaseinmacro-averageF1,comparedtojust
0.65inthebestcaseforNomBank.
77.3 GeneralizationonUnseenCompounds
Wenowanalyzethemodels’abilitytogeneralizetocompoundsnotseenduringtraining. Recent
research suggests that gains in noun-noun compound interpretation using word embeddings and
similarneuralclassificationmodelsmightbeduetolexicalmemorization. Inotherwords,themodels
learn that specific nouns are strong indicators of specific relations. To assess the role of lexical
memorizationinourmodels,wequantifythenumberofunseencompoundsthattheSTL,TL,and
MTLmodelspredictcorrectly.
We differentiate between ’partly’ and ’completely’ unseen compounds. A compound is ’partly’
unseen if one of its constituents (left or right) is not present in the training data. A ’completely’
unseencompoundisonewhereneithertheleftnortherightconstituentappearsinthetrainingdata.
Overall,nearly20%ofthecompoundsinthetestsplithaveanunseenleftconstituent,about16%
haveanunseenrightconstituent,and4%arecompletelyunseen. Table9comparestheperformance
ofthedifferentmodelsonthesethreegroupsintermsoftheproportionofcompoundsmisclassified
ineachgroup.
Table9: Generalizationerroronthesubsetofunseencompoundsinthetestsplit. L:Leftconstituent.
R:Rightconstituent. L&R:Completelyunseen.
NomBank PCEDT
Model L R L&R L R L&R
Count 351 286 72 351 286 72
STL 27.92 39.51 50.00 45.01 47.55 41.67
TLE 25.93 36.71 48.61 43.87 47.55 41.67
TLH 26.21 38.11 50.00 46.15 49.30 47.22
TLEH 26.50 38.81 52.78 45.87 47.55 43.06
MTLE 24.50 33.22 38.89 44.44 47.20 43.06
MTLF 22.79 34.27 40.28 44.16 47.90 38.89
Table 9 shows that Transfer Learning (TL) and Multi-Task Learning (MTL) approaches reduce
generalization error in NomBank across all scenarios, with the exception of TLH and TLEH for
completelyunseencompounds,whereerrorincreases. Thegreatesterrorreductionsareachieved
byMTLmodelsacrossallthreetypesofunseencompounds. Specifically,MTLEreducestheerror
byapproximatelysixpointsforcompoundswithunseenrightconstituentsandbyelevenpointsfor
fullyunseencompounds. Moreover,MTLFreducestheerrorbyfivepointswhentheleftconstituent
isunseen. It’simportanttointerprettheseresultsinconjunctionwiththeCountrowinTable9for
a comprehensive view. For example, the eleven-point error decrease in fully unseen compounds
representseightcompounds. InPCEDT,thelargesterrorreductionisonunseenleftconstituents,
whichisabout1.14points,correspondingtofourcompounds;it’s0.35onunseenrightconstituents
(onecompound)and2.7onfullyunseencompounds,ortwocompounds.
Uponmanualinspectionofcompoundsthatledtosubstantialreductionsinthegeneralizationerror,
specificallywithinNomBank,weexaminedthedistributionofrelationswithincorrectlypredicted
unseen compound sets. Compared to the STL model, MTLE reduces generalization error for
completelyunseencompoundsbyatotalofeightcompounds,ofwhichsevenareannotatedwiththe
relationARG1,whichisthemostcommoninNomBank. Regardingtheunseenrightconstituents,
MTLE’s 24 improved compounds consist of 18 ARG1, 5 ARG0, and 1 ARG2 compounds. A
similarpatternariseswhenexaminingTLEmodelimprovements,wheremostgainscomefrombetter
predictionsofARG1andARG0relations.
Alargeportionofunseencompounds,whetherpartlyorentirelyunseen,thatweremisclassifiedby
everymodel,werenotoftypeARG1inNomBank,orRSTRinPCEDT.Thispattern,alongwith
correctlypredictedunseencompoundsprimarilyannotatedwiththemostcommonrelations,suggests
thatclassificationmodelsrelyonlexicalmemorizationtolearnthecompoundrelationinterpretation.
Tobettercomprehendlexicalmemorization’simpact,wepresenttheratioofrelation-specificcon-
stituents in both NomBank and PCEDT, as depicted in Figure 2. We define a relation-specific
constituentasaleftorrightconstituentthatappearswithonlyonespecificrelationwithinthetraining
data. Its ratio is calculated as its proportion in the full set of left or right constituents for each
8relation. AnalyzingFigure2revealsthatNomBankrelationspossesshigherratiosofrelation-specific
constituentscomparedtoPCEDT.Thispotentiallymakeslearningtheformereasierifthemodel
solely relies on lexical memorization. Additionally, ARGM-TMP in NomBank and TWHEN in
PCEDT have distinctly high ratios compared to other relations in Figure 2. These relations also
havethesecond-highestF1scoreintheirdatasets—exceptforSTLonPCEDT(seeTables4and
5). LexicalmemorizationisthereforealikelycauseofthesehighF1scores. Wealsoobservedthat
lowerratiosofrelation-specificconstituentscorrelatewithlowerF1scores,suchasAPPandREGin
PCEDT.Basedontheseinsights,wecan’tdismissthepossibilitythatourmodelsshowsomedegree
oflexicalmemorization,despitemanualanalysisalsopresentingcaseswheremodelsdemonstrate
generalizationandcorrectpredictionsinsituationswherelexicalmemorizationisimpossible.
8 Conclusion
Theapplicationoftransferandmulti-tasklearninginnaturallanguageprocessinghasgainedsig-
nificanttraction,yetconsiderableambiguitypersistsregardingtheeffectivenessofparticulartask
characteristicsandexperimentalsetups. ThisresearchendeavorstoclarifythebenefitsofTLand
MTLinthecontextofsemanticinterpretationofnoun-nouncompounds. Byexecutingasequenceof
minimallycontrastingexperimentsandconductingthoroughanalysisofresultsandpredictionerrors,
wedemonstratehowbothTLandMTLcanmitigatetheeffectsofclassimbalanceanddrastically
enhancepredictionsforlow-frequencyrelations. Overall,ourTL,andparticularlyourMTLmodels,
arebetteratmakingpredictionsbothquantitativelyandqualitatively. Notably,theimprovementsare
observedonthe’mostchallenging’inputsthatincludeatleastoneconstituentthatwasnotpresentin
thetrainingdata. However,clearindicationsof’lexicalmemorization’effectsareevidentinourerror
analysisofunseencompounds.
Typically,thetransferofrepresentationsorsharingbetweentasksismoreeffectiveattheembedding
layers,whichrepresentthemodel’sinternalrepresentationofthecompoundconstituents.Furthermore,
inmulti-tasklearning,thecompletesharingofmodelarchitectureacrosstasksdegradesitscapacity
togeneralizewhenitcomestolessfrequentrelations.
ThedatasetprovidedbyFares(2016)isanappealingresourcefornewneuralapproachestocompound
interpretation because it links this sub-problem with broad-coverage semantic role labeling or
semanticdependencyparsinginPCEDTandNomBank. Futureresearchwillfocusonincorporating
additionalnaturallanguageprocessingtasksdefinedusingtheseframeworkstounderstandnoun-noun
compoundinterpretationusingTLandMTL.
9