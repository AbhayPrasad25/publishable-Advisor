Addressing Popularity Bias with Popularity-Conscious Alignment and
Contrastive Learning
Abstract
CollaborativeFiltering(CF)oftenencounterssubstantialdifficultieswithpopularitybiasbecauseoftheskewed
distributionofitemsinreal-worlddatasets. Thistendencycreatesanotabledifferenceinaccuracybetweenitems
thatarepopularandthosethatarenot. Thisdiscrepancyimpedestheaccuratecomprehensionofuserpreferences
andintensifiestheMattheweffectwithinrecommendationsystems. Tocounterpopularitybias,currentmethods
concentrateonhighlightinglesspopularitemsorondifferentiatingthecorrelationbetweenitemrepresentations
and their popularity. Despite their effectiveness, current approaches continue to grapple with two significant
issues: firstly,theextractionofsharedsupervisorysignalsfrompopularitemstoenhancetherepresentationsof
lesspopularitems,andsecondly,thereductionofrepresentationseparationcausedbypopularitybias. Inthis
study,wepresentanempiricalexaminationofpopularitybiasandintroduceamethodcalledPopularity-Aware
AlignmentandContrast(PAAC)totacklethesetwoproblems. Specifically,weutilizethecommonsupervisory
signalsfoundinpopularitemrepresentationsandintroduceaninnovativepopularity-awaresupervisedalignment
moduletoimprovethelearningofrepresentationsforunpopularitems. Furthermore,weproposeadjustingthe
weightsinthecontrastivelearninglosstodecreasetheseparationofrepresentationsbyfocusingonpopularity.
WeconfirmtheefficacyandlogicofPAACinreducingpopularitybiasthroughthoroughexperimentsonthree
real-worlddatasets.
1 Introduction
Contemporaryrecommendersystemsareessentialinreducinginformationoverload. Personalizedrecommendationsfrequently
employ collaborative filtering (CF) to assist users in discovering items that may interest them. CF-based techniques primarily
learnuserpreferencesanditemattributesbymatchingtherepresentationsofuserswiththeitemstheyengagewith. Despitetheir
achievements, CF-based methods frequently encounter the issue of popularity bias, which leads to considerable disparities in
accuracybetweenitemsthatarepopularandthosethatarenot. Popularitybiasoccursbecausetherearelimitedsupervisorysignals
foritemsthatarenotpopular,whichresultsinoverfittingduringthetrainingphaseanddecreasedeffectivenessonthetestset. This
hinderstheprecisecomprehensionofuserpreferences,therebydiminishingthevarietyofrecommendations. Furthermore,popularity
biascanworsentheMattheweffect,whereitemsthatarealreadypopulargainevenmorepopularitybecausetheyarerecommended
morefrequently.
Twosignificantchallengesarepresentedwhenmitigatingpopularitybiasinrecommendationsystems. Thefirstchallengeisthe
inadequaterepresentationofunpopularitemsduringtraining,whichresultsinoverfittingandlimitedgeneralizationability. The
secondchallenge,knownasrepresentationseparation,happenswhenpopularandunpopularitemsarecategorizedintodistinct
semanticspaces,therebyintensifyingthebiasanddiminishingtheprecisionofrecommendations.
2 Methodology
Toovercomethecurrentdifficultiesinreducingpopularitybias,weintroducethePopularity-AwareAlignmentandContrast(PAAC)
method. Weutilizethecommonsupervisorysignalspresentinpopularitemrepresentationstodirectthelearningofunpopular
representations,andwepresentapopularity-awaresupervisedalignmentmodule. Moreover,weincorporateare-weightingsystem
inthecontrastivelearningmoduletodealwithrepresentationseparationbyconsideringpopularity.
2.1 SupervisedAlignmentModule
Duringthetrainingprocess,thealignmentofrepresentationsusuallyemphasizesusersanditemsthathaveinteracted,oftencausing
itemstobeclosertointeractedusersthannon-interactedonesintherepresentationspace. However,becauseunpopularitemshave
limitedinteractions,theyareusuallymodeledbasedonasmallgroupofusers. Thislimitedfocuscanresultinoverfitting,asthe
representationsofunpopularitemsmightnotfullycapturetheirfeatures.Thedisparityinthequantityofsupervisorysignalsisessentialforlearningrepresentationsofbothpopularandunpopularitems.
Specifically, popular items gain from a wealth of supervisory signals during the alignment process, which helps in effectively
learningtheirrepresentations. Ontheotherhand,unpopularitems,whichhavealimitednumberofusersprovidingsupervision,are
moresusceptibletooverfitting. Thisisbecausethereisinsufficientrepresentationlearningforunpopularitems,emphasizingthe
effectofsupervisorysignaldistributiononthequalityofrepresentation. Intuitively,itemsinteractedwithbythesameuserhave
somesimilarcharacteristics. Inthissection,weutilizecommonsupervisorysignalsinpopularitemrepresentationsandsuggesta
popularity-awaresupervisedalignmentmethodtoimprovetherepresentationsofunpopularitems.
Weinitiallyfilteritemswithsimilarcharacteristicsbasedontheuser’sinterests. Foranyuser,wedefinethesetofitemstheyinteract
with. Wecountthefrequencyofeachitemappearinginthetrainingdatasetasitspopularity. Subsequently,wegroupitemsbasedon
theirrelativepopularity. Wedivideitemsintotwogroups: thepopularitemgroupandtheunpopularitemgroup. Thepopularityof
eachiteminthepopulargroupishigherthanthatofanyitemintheunpopulargroup. Thisindicatesthatpopularitemsreceivemore
supervisoryinformationthanunpopularitems,resultinginpoorerrecommendationperformanceforunpopularitems.
Totackletheissueofinsufficientrepresentationlearningforunpopularitems,weutilizetheconceptthatitemsinteractedwithbythe
sameusersharesomesimilarcharacteristics. Specifically,weusesimilarsupervisorysignalsinpopularitemrepresentationsto
improvetherepresentationsofunpopularitems. Wealigntherepresentationsofitemstoprovidemoresupervisoryinformationto
unpopularitemsandimprovetheirrepresentation,asfollows:
(cid:88) 1 (cid:88)
L = ||f(i)−f(j)|| , (1)
SA |I | 2
u
u∈U i∈Iu ,j∈Iu
pop unpop
wheref(·)isarecommendationencoderandh =f(i). Byefficientlyusingtheinherentinformationinthedata,weprovidemore
i
supervisorysignalsforunpopularitemswithoutintroducingadditionalsideinformation. Thismoduleenhancestherepresentationof
unpopularitems,mitigatingtheoverfittingissue.
2.2 Re-weightingContrastModule
Recentresearchhasindicatedthatpopularitybiasfrequentlyleadstoanoticeableseparationintherepresentationofitemembeddings.
Althoughmethodsbasedoncontrastivelearningaimtoenhanceoveralluniformitybydistancingnegativesamples,theircurrent
samplingmethodsmightunintentionallyworsenthisseparation. Whennegativesamplesfollowthepopularitydistribution,which
isdominatedbypopularitems,prioritizingunpopularitemsaspositivesampleswidensthegapbetweenpopularandunpopular
itemsintherepresentationspace. Conversely,whennegativesamplesfollowauniformdistribution,focusingonpopularitems
separatesthemfrommostunpopularones,thusworseningtherepresentationgap. Existingstudiesusethesameweightsforpositive
andnegativesamplesinthecontrastivelossfunction,withoutconsideringdifferencesinitempopularity. However,inreal-world
recommendationdatasets,theimpactofitemsvariesduetodatasetcharacteristicsandinteractiondistributions. Neglectingthis
aspectcouldleadtosuboptimalresultsandexacerbaterepresentationseparation.
We propose to identify different influences by re-weighting different popularity items. To this end, we introduce re-weighting
differentpositiveandnegativesamplestomitigaterepresentationseparationfromapopularity-centricperspective. Weincorporate
thisapproachintocontrastivelearningtobetteroptimizetheconsistencyofrepresentations. Specifically,weaimtoreducetherisk
ofpushingitemswithvaryingpopularityfurtherapart. Forexample,whenusingapopularitemasapositivesample,ourgoalis
toavoidpushingunpopularitemstoofaraway. Thus,weintroducetwohyperparameterstocontroltheweightswhenitemsare
consideredpositiveandnegativesamples.
Toensurebalancedandequitablerepresentationsofitemswithinourmodel,wefirstproposeadynamicstrategytocategorizeitems
intopopularandunpopulargroupsforeachmini-batch. Insteadofrelyingonafixedglobalthreshold,whichoftenleadstothe
overrepresentationofpopularitemsacrossvariousbatches,weimplementahyperparameterx. Thishyperparameterreadjuststhe
classificationofitemswithinthecurrentbatch. Byadjustingthehyperparameterx,wemaintainabalancebetweendifferentitem
popularitylevels. Thisenhancesthemodel’sabilitytogeneralizeacrossdiverseitemsetsbyaccuratelyreflectingthepopularity
distributioninthecurrenttrainingcontext. Specifically,wedenotethesetofitemswithineachbatchasI . AndthenwedivideI
B B
intoapopulargroupI andanunpopulargroupI basedontheirrespectivepopularitylevels,classifyingthetopx%ofitems
pop unpop
asI :
pop
I =I ∪I ,∀i∈I ∧j ∈I ,p(i)>p(j), (2)
B pop unpop pop unpop
whereI ∈I andI ∈I aredisjoint,withI consistingofthetopx%ofitemsinthebatch.Inthiswork,wedynamically
pop B unpop B pop
divideditemsintopopularandunpopulargroupswithineachmini-batchbasedontheirpopularity,assigningthetop50%aspopular
itemsandthebottom50%asunpopularitems. Thisradionotonlyensuresequalrepresentationofbothgroupsinourcontrastive
learningbutalsoallowsitemstobeclassifiedadaptivelybasedonthebatch’scurrentcomposition.
Afterthat,weuseInfoNCEtooptimizetheuniformityofitemrepresentations. UnliketraditionalCL-basedmethods,wecalculate
thelossfordifferentitemgroups. Specifically,weintroducethehyperparameterαtocontrolthepositivesampleweightsbetween
popularandunpopularitems,adaptingtovaryingitemdistributionsindifferentdatasets:
2LCL =α×LCL +(1−α)×LCL , (3)
item pop unpop
where LCL represents the contrastive loss when popular items are considered as positive samples, and LCL represents the
pop unpop
contrastivelosswhenunpopularitemsareconsideredaspositivesamples. Thevalueofαrangesfrom0to1,whereα=0means
exclusive emphasis on the loss of unpopular items LCL , and α = 1 means exclusive emphasis on the loss of popular items
unpop
LCL. Byadjustingα,wecaneffectivelybalancetheimpactofpositivesamplesfrombothpopularandunpopularitems,allowing
pop
adaptabilitytovaryingitemdistributionsindifferentdatasets.
Followingthis,wefine-tunetheweightingofnegativesamplesinthecontrastivelearningframeworkusingthehyperparameterβ.
Thisparametercontrolshowsamplesfromdifferentpopularitygroupscontributeasnegativesamples. Specifically,weprioritize
re-weightingitemswithpopularityoppositetothepositivesamples,mitigatingtheriskofexcessivelypushingnegativesamples
awayandreducingrepresentationseparation. Simultaneously,thisapproachensurestheoptimizationofintra-groupconsistency. For
instance,whendealingwithpopularitemsaspositivesamples,weseparatelycalculatetheimpactofpopularandunpopularitems
asnegativesamples. Thehyperparameterβ isthenusedtocontrolthedegreetowhichunpopularitemsarepushedaway. Thisis
formalizedasfollows:
L′
=
(cid:88)
log
exp(h′ ih i/τ)
, (4)
pop (cid:80) exp(h′h /τ)+β(cid:80) exp(h′h /τ)
i∈Ipop j∈Ipop i j j∈Iunpop i j
similarly,thecontrastivelossforunpopularitemsisdefinedas:
L′
=
(cid:88)
log
exp(h′ ih i/τ)
, (5)
unpop (cid:80) exp(h′h /τ)+β(cid:80) exp(h′h /τ)
i∈Iunpop j∈Iunpop i j j∈Ipop i j
wheretheparameterβ rangesfrom0to1,controllingthenegativesampleweightinginthecontrastiveloss. Whenβ =0,itmeans
thatonlyintra-groupuniformityoptimizationisperformed. Conversely,whenβ =1,itmeansequaltreatmentofbothpopularand
unpopularitemsintermsoftheirimpactonpositivesamples. Thesettingofβ allowsforaflexibleadjustmentbetweenprioritizing
intra-group uniformity and considering the impact of different popularity levels in the training. We prefer to push away items
withinthesamegrouptooptimizeuniformity. Thissetuphelpspreventover-optimizingtheuniformityofdifferentgroups,thereby
mitigatingrepresentationseparation.
Thefinalre-weightingcontrastiveobjectiveistheweightedsumoftheuserobjectiveandtheitemobjective:
1
L = ×(LCL +LCL ). (6)
CL 2 item user
Inthisway,wenotonlyachievedconsistencyinrepresentationbutalsoreducedtheriskoffurtherseparatingitemswithsimilar
characteristicsintodifferentrepresentationspaces,therebyalleviatingtheissueofrepresentationseparationcausedbypopularity
bias.
2.3 ModelOptimization
Toreducepopularitybiasincollaborativefilteringtasks,weemployamulti-tasktrainingstrategytojointlyoptimizetheclassic
recommendationloss(L ),supervisedalignmentloss(L ),andre-weightingcontrastloss(L ).
REC SA CL
L=L +λ L +λ L +λ ||Θ||2, (7)
REC 1 SA 2 CL 3
whereΘisthesetofmodelparametersinL aswedonotintroduceadditionalparameters,λ andλ arehyperparametersthat
REC 1 2
controlthestrengthsofthepopularity-awaresupervisedalignmentlossandthere-weightingcontrastivelearninglossrespectively,
andλ istheL regularizationcoefficient. Aftercompletingthemodeltrainingprocess,weusethedotproducttopredictunknown
3 2
preferencesforrecommendations.
3 Experiments
In this section, we assess the efficacy of PAAC through comprehensive experiments, aiming to address the following research
questions:
• HowdoesPAACcomparetoexistingdebiasingmethods?
• HowdodifferentdesignedcomponentsplayrolesinourproposedPAAC?
3• HowdoesPAACalleviatethepopularitybias?
• Howdodifferenthyper-parametersaffectthePAACrecommendationperformance?
3.1 ExperimentsSettings
3.1.1 Datasets
Inourexperiments,weusethreewidelypublicdatasets: Amazon-book,Yelp2018,andGowalla. Weretainedusersanditemswitha
minimumof10interactions.
3.1.2 BaselinesandEvaluationMetrics
We implement the state-of-the-art LightGCN to instantiate PAAC, aiming to investigate how it alleviates popularity bias. We
comparePAACwithseveraldebiasedbaselines,includingre-weighting-basedmodels,decorrelation-basedmodels,andcontrastive
learning-basedmodels.
Weutilizethreewidelyusedmetrics,namelyRecall@K,HR@K,andNDCG@K,toevaluatetheperformanceofTop-Krecommen-
dation. Recall@KandHR@Kassessthenumberoftargetitemsretrievedintherecommendationresults,emphasizingcoverage. In
contrast,NDCG@Kevaluatesthepositionsoftargetitemsintherankinglist,withafocusontheirpositionsinthelist. Weuse
thefullrankingstrategy,consideringallnon-interacteditemsascandidateitemstoavoidselectionbiasduringtheteststage. We
repeatedeachexperimentfivetimeswithdifferentrandomseedsandreportedtheaveragescores.
3.2 OverallPerformance
AsshowninTable1,wecompareourmodelwithseveralbaselinesacrossthreedatasets. Thebestperformanceforeachmetric
ishighlightedinbold,whilethesecondbestisunderlined. Ourmodelconsistentlyoutperformsallcomparedmethodsacrossall
metricsineverydataset.
• OurproposedmodelPAACconsistentlyoutperformsallbaselinesandsignificantlymitigatesthepopularitybias. Specif-
ically,PAACenhancesLightGCN,achievingimprovementsof282.65%,180.79%,and82.89%inNDCG@20onthe
Yelp2018,Gowalla,andAmazon-Bookdatasets,respectively. Comparedtothestrongestbaselines,PAACdeliversbetter
performance. ThemostsignificantimprovementsareobservedonYelp2018,whereourmodelachievesan8.70%increase
inRecall@20,a10.81%increaseinHR@20,anda30.2%increaseinNDCG@20. Thisimprovementcanbeattributed
toouruseofpopularity-awaresupervisedalignmenttoenhancetherepresentationoflesspopularitemsandre-weighted
contrastivelearningtoaddressrepresentationseparationfromapopularity-centricperspective.
• The performance improvements of PAAC are smaller on sparser datasets. For example, on the Gowalla dataset, the
improvementsinRecall@20,HR@20,andNDCG@20are3.18%,5.85%,and5.47%,respectively. Thismaybebecause,
insparserdatasetslikeGowalla,evenpopularitemsarenotwell-representedduetolowerdatadensity. Aligningunpopular
itemswiththesepoorlyrepresentedpopularitemscanintroducenoiseintothemodel. Therefore,thebenefitsofusing
supervisorysignalsforunpopularitemsmaybereducedinverysparseenvironments, leadingtosmallerperformance
improvements.
• Regardingthebaselinesformitigatingpopularitybias,theimprovementofsomeisrelativelylimitedcomparedtothe
backbonemodel(LightGCN)andevenperformsworseinsomecases. Thismaybebecausesomearespecificallydesigned
fortraditionaldata-splittingscenarios,wherethetestsetstillfollowsalong-taildistribution,leadingtopoorgeneralization.
Somemitigatepopularitybiasbyexcludingitempopularityinformation. Othersuseinvariantlearningtoremovepopularity
information at the representation level, generally performing better than the formers. This shows the importance of
addressingpopularitybiasattherepresentationlevel. Someoutperformtheotherbaselines,emphasizingthenecessaryto
improveitemrepresentationconsistencyformitigatingpopularitybias.
• Differentmetricsacrossvariousdatasetsshowvaryingimprovementsinmodelperformance. Thissuggeststhatdifferent
debiasingmethodsmayneeddistinctoptimizationstrategiesformodels. Additionally,weobservevaryingeffectsofPAAC
acrossdifferentdatasets. ThisdifferencecouldbeduetothesparsernatureoftheGowalladataset. Conversely,ourmodel
candirectlyprovidesupervisorysignalsforunpopularitemsandconductintra-groupoptimization,consistentlymaintaining
optimalperformanceacrossallmetricsonthethreedatasets.
3.3 AblationStudy
TobetterunderstandtheeffectivenessofeachcomponentinPAAC,weconductablationstudiesonthreedatasets. Table2presentsa
comparisonbetweenPAACanditsvariantsonrecommendationperformance. Specifically,PAAC-w/oPreferstothevariantwhere
there-weightingcontrastivelossofpopularitemsisremoved,focusinginsteadonoptimizingtheconsistencyofrepresentationsfor
unpopularitems. Similarly,PAAC-w/oUdenotestheremovalofthere-weightingcontrastivelossforunpopularitems. PAAC-w/o
Areferstothevariantwithoutthepopularity-awaresupervisedalignmentloss. It’sworthnotingthatPAAC-w/oAdiffersfrom
4Table 1: Performance comparison on three public datasets with K = 20. The best performance is indicated in bold, while the
second-bestperformanceisunderlined. Thesuperscripts*indicatep≤0.05forthepairedt-testofPAACvs. thebestbaseline(the
relativeimprovementsaredenotedasImp.).
Yelp2018 Gowalla Amazon-book
Model
Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20
MF 0.0050 0.0109 0.0093 0.0343 0.0422 0.0280 0.0370 0.0388 0.0270
LightGCN 0.0048 0.0111 0.0098 0.0380 0.0468 0.0302 0.0421 0.0439 0.0304
IPS 0.0104 0.0183 0.0158 0.0562 0.0670 0.0444 0.0488 0.0510 0.0365
!MACR 0.0402 0.0312 0.0265 0.0908 0.1086 0.0600 0.0515 0.0609 0.0487
α-Adjnorm 0.0053 0.0088 0.0080 0.0328 0.0409 0.0267 0.0422 0.0450 0.0264
InvCF 0.0444 0.0344 0.0291 0.1001 0.1202 0.0662 0.0562 0.0665 0.0515
Adap-τ 0.0450 0.0497 0.0341 0.1182 0.1248 0.0794 0.0641 0.0678 0.0511
SimGCL 0.0449 0.0518 0.0345 0.1194 0.1228 0.0804 0.0628 0.0648 0.0525
PAAC 0.0494* 0.0574* 0.0375* 0.1232* 0.1321* 0.0848* 0.0701* 0.0724* 0.0556*
Imp. +9.78% +10.81% +8.70% +3.18% +5.85% +5.47% +9.36% +6.78% 5.90%
SimGCLinthatwesplitthecontrastivelossontheitemside,LCL ,intotwodistinctlosses: LCL andLCL . Thisapproach
item pop unpop
allowsustoseparatelyaddresstheconsistencyofpopularandunpopularitemrepresentations,therebyprovidingamoredetailed
analysisoftheimpactofeachcomponentontheoverallperformance.
FromTable2,weobservethatPAAC-w/oAoutperformsSimGCLinmostcases. Thisvalidatesthatre-weightingtheimportanceof
popularandunpopularitemscaneffectivelyimprovethemodel’sperformanceinalleviatingpopularitybias. Italsodemonstratesthe
effectivenessofusingsupervisionsignalsfrompopularitemstoenhancetherepresentationsofunpopularitems,providingmore
opportunitiesforfutureresearchonmitigatingpopularitybias. Moreover,comparedwithPAAC-w/oU,PAAC-w/oPresultsinmuch
worseperformance. Thisconfirmstheimportanceofre-weightingpopularitemsincontrastivelearningformitigatingpopularity
bias. Finally,PAACconsistentlyoutperformsthethreevariants,demonstratingtheeffectivenessofcombiningsupervisedalignment
andre-weightingcontrastivelearning. Basedontheaboveanalysis,weconcludethatleveragingsupervisorysignalsfrompopular
itemrepresentationscanbetteroptimizerepresentationsforunpopularitems,andre-weightingcontrastivelearningallowsthemodel
tofocusonmoreinformativeorcriticalsamples,therebyimprovingoverallperformance. Alltheproposedmodulessignificantly
contributetoalleviatingpopularitybias.
Table 2: Ablation study of PAAC, highlighting the best-performing model on each dataset and metrics in bold. Specifically,
PAAC-w/oPremovesthere-weightingcontrastivelossofpopularitems,PAAC-w/oUeliminatesthere-weightingcontrastiveloss
ofunpopularitems,andPAAC-w/oAomitsthepopularity-awaresupervisedalignmentloss.
Yelp2018 Gowalla Amazon-book
Model
Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20
SimGCL 0.0449 0.0518 0.0345 0.1194 0.1228 0.0804 0.0628 0.0648 0.0525
!
PAAC-w/oP 0.0443 0.0536 0.0340 0.1098 0.1191 0.0750 0.0616 0.0639 0.0458
PAAC-w/oU 0.0462 0.0545 0.0358 0.1120 0.1179 0.0752 0.0594 0.0617 0.0464
PAAC-w/oA 0.0466 0.0547 0.0360 0.1195 0.1260 0.0815 0.0687 0.0711 0.0536
PAAC 0.0494* 0.0574* 0.0375* 0.1232* 0.1321* 0.0848* 0.0701* 0.0724* 0.0556*
3.4 DebiasAbility
TofurtherverifytheeffectivenessofPAACinalleviatingpopularitybias,weconductacomprehensiveanalysisfocusingonthe
recommendationperformanceacrossdifferentpopularityitemgroups. Specifically,20%ofthemostpopularitemsarelabeled
’Popular’,andtherestarelabeled’Unpopular’. WecomparetheperformanceofPAACwithLightGCN,IPS,MACR,andSimGCL
usingtheNDCG@20metricacrossdifferentpopularitygroups. Weuse∆todenotetheaccuracygapbetweenthetwogroups. We
drawthefollowingconclusions:
• Improving theperformanceof unpopularitems iscrucialfor enhancingoverallmodelperformance. Specially, on the
Yelp2018dataset,PAACshowsreducedaccuracyinrecommendingpopularitems,withanotabledecreaseof20.14%
comparedtoSimGCL.However,despitethisdecrease,theoverallrecommendationaccuracysurpassesthatofSimGCL
by11.94%,primarilyduetoa6.81%improvementinrecommendingunpopularitems. Thisimprovementhighlightsthe
importanceofbetterrecommendationsforunpopularitemsandemphasizestheircrucialroleinenhancingoverallmodel
performance.
5• OurproposedPAACsignificantlyenhancestherecommendationperformanceforunpopularitems. Specifically,weobserve
animprovementof8.94%and7.30%inNDCG@20relativetoSimGCLontheGowallaandYelp2018datasets,respectively.
Thisimprovementisduetothepopularity-awarealignmentmethod,whichusessupervisorysignalsfrompopularitemsto
improvetherepresentationsofunpopularitems.
• PAAChassuccessfullynarrowedtheaccuracygapbetweendifferentitemgroups. Specifically,PAACachievedthesmallest
gap,reducingtheNDCG@20accuracygapby34.18%and87.50%ontheGowallaandYelp2018datasets,respectively.
Thisindicatesthatourmethodtreatsitemsfromdifferentgroupsfairly,effectivelyalleviatingtheimpactofpopularity
bias. Thissuccesscanbeattributedtoourre-weightedcontrastmodule,whichaddressesrepresentationseparationfroma
popularity-centricperspective,resultinginmoreconsistentrecommendationresultsacrossdifferentgroups.
3.5 HyperparameterSensitivities
In this section, we analyze the impact of hyperparameters in PAAC. Firstly, we investigate the influence of λ and λ , which
1 2
respectivelycontroltheimpactofthepopularity-awaresupervisedalignmentandre-weightingcontrastloss. Additionally,inthe
re-weightingcontrastiveloss,weintroducetwohyperparameters,αandβ,tocontrolthere-weightingofdifferentpopularityitems
aspositiveandnegativesamples. Finally,weexploretheimpactofthegroupingratioxonthemodel’sperformance.
3.5.1 Effectofλ andλ
1 2
AsformulatedinEq. (11),λ controlstheextentofprovidingadditionalsupervisorysignalsforunpopularitems,whileλ controls
1 2
theextentofoptimizingrepresentationconsistency. Horizontally,withtheincreaseinλ ,theperformanceinitiallyincreasesand
2
thendecreases. Thisindicatesthatappropriatere-weightingcontrastivelosseffectivelyenhancestheconsistencyofrepresentation
distributions,mitigatingpopularitybias. However,overlystrongcontrastivelossmayleadthemodeltoneglectrecommendation
accuracy. Vertically, as λ increases, the performance also initially increases and then decreases. This suggests that suitable
1
alignmentcanprovidebeneficialsupervisorysignalsforunpopularitems,whiletoostronganalignmentmayintroducemorenoise
frompopularitemstounpopularones,therebyimpactingrecommendationperformance.
3.5.2 Effectofre-weightingcoefficientαandβ
Tomitigaterepresentationseparationduetoimbalancedpositiveandnegativesampling,weintroducetwohyperparametersintothe
contrastiveloss. Specifically,αcontrolstheweightdifferencebetweenpositivesamplesfrompopularandunpopularitems,whileβ
controlstheinfluenceofdifferentpopularityitemsasnegativesamples.
Inourexperiments,whilekeepingotherhyperparametersconstant,wesearchαandβ withintherange{0,0.2,0.4,0.6,0.8,1}. As
αandβ increase,performanceinitiallyimprovesandthendeclines. TheoptimalhyperparametersfortheYelp2018andGowalla
datasetsareα=0.8,β =0.6andα=0.2,β =0.2,respectively. Thismaybeattributedtothecharacteristicsofthedatasets. The
Yelp2018dataset,withahigheraverageinteractionfrequencyperitem,benefitsmorefromahigherweightαforpopularitemsas
positivesamples. Conversely,theGowalladataset,beingrelativelysparse,prefersasmallerα. Thisindicatestheimportanceof
consideringdatasetcharacteristicswhenadjustingthecontributionsofpopularandunpopularitemstothemodel.
Notably,αandβ arenothighlysensitivewithintherange[0,1],performingwellacrossabroadspectrum. Performanceexceedsthe
baselineregardlessofβ valueswhenotherparametersareoptimal. Additionally,αvaluesfrom[0.4,1.0]ontheYelp2018dataset
and[0.2,0.8]ontheGowalladatasetsurpassthebaseline,indicatinglessneedforprecisetuning. Thus,αandβ achieveoptimal
performancewithoutmeticulousadjustments,focusingonweightcoefficientstomaintainmodelefficacy.
3.5.3 Effectofgroupingratiox
To investigate the impact of different grouping ratios on recommendation performance, we developed a flexible classification
methodforitemswithineachmini-batchbasedontheirpopularity. Insteadofadoptingafixedglobalthreshold,whichtendsto
overrepresentpopularitemsinsomemini-batches,ourapproachdynamicallydividesitemsineachmini-batchintopopularand
unpopularcategories. Specifically,thetopx%ofitemsareclassifiedaspopularandtheremaining(100-x)%asunpopular,withx
varying. Thisstrategypreventstheoverrepresentationtypicalinfixeddistributionmodels,whichcouldskewthelearningprocess
anddegradeperformance. Toquantifytheeffectsofthesevaryingratios,weexaminedvariousdivisionratiosforpopularitems,
including20%,40%,60%,and80%,asshowninTable3. Thepreliminaryresultsindicatethatbothextremelylowandhighratios
negativelyaffectmodelperformance,therebyunderscoringthesuperiorityofourdynamicdatapartitioningapproach. Moreover,
withinthe40%-60%range,ourmodel’sperformanceremainedconsistentlyrobust,furthervalidatingtheeffectivenessofPAAC.
6Table3: Performancecomparisonacrossvaryingpopularitemratiosxonmetrics.
Yelp2018 Gowalla
Ratio
Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20
20% 0.0467 0.0555 0.0361 0.1232 0.1319 0.0845
!
40% 0.0505 0.0581 0.0378 0.1239 0.1325 0.0848
50% 0.0494 0.0574 0.0375 0.1232 0.1321 0.0848
60% 0.0492 0.0569 0.0370 0.1225 0.1314 0.0843
80% 0.0467 0.0545 0.0350 0.1176 0.1270 0.0818
4 RelatedWork
4.1 PopularityBiasinRecommendation
Popularitybiasisaprevalentprobleminrecommendersystems,whereunpopularitemsinthetrainingdatasetareseldomrecom-
mended. Numeroustechniqueshavebeensuggestedtoexamineanddecreaseperformancevariationsbetweenpopularandunpopular
items. Thesetechniquescanbebroadlydividedintothreecategories.
• Re-weighting-basedmethodsaimtoincreasethetrainingweightorscoresforunpopularitems,redirectingfocusaway
frompopularitemsduringtrainingorprediction. Forinstance,IPSaddscompensationtounpopularitemsandadjusts
the prediction of the user-item preference matrix, resulting in higher preference scores and improving rankings for
unpopularitems. α-AdjNormenhancesthefocusonunpopularitemsbycontrollingthenormalizationstrengthduringthe
neighborhoodaggregationprocessinGCN-basedmodels.
• Decorrelation-basedmethodsaimtoeffectivelyremovethecorrelationsbetweenitemrepresentations(orpredictionscores)
andpopularity. Forinstance,MACRusescounterfactualreasoningtoeliminatethedirectimpactofpopularityonitem
outcomes. Incontrast,InvCFoperatesontheprinciplethatitemrepresentationsremaininvarianttochangesinpopularity
semantics,filteringoutunstableoroutdatedpopularitycharacteristicstolearnunbiasedrepresentations.
• Contrastive-learning-basedmethodsaimtoachieveoveralluniformityinitemrepresentationsusingInfoNCE,preserving
moreinherentcharacteristicsofitemstomitigatepopularitybias. Thisapproachhasbeendemonstratedasastate-of-the-art
methodforalleviatingpopularitybias. Itemploysdataaugmentationtechniquessuchasgraphaugmentationorfeature
augmentationtogeneratedifferentviews,maximizingpositivepairconsistencyandminimizingnegativepairconsistency
topromotemoreuniformrepresentations. Specifically,Adap-τ adjustsuser/itemembeddingstospecificvalues,while
SimGCLintegratesInfoNCElosstoenhancerepresentationuniformityandalleviatepopularitybias.
4.2 RepresentationLearningforCF
Representationlearningiscrucialinrecommendationsystems, especiallyinmoderncollaborativefiltering(CF)techniques. It
createspersonalizedembeddingsthatcaptureuserpreferencesanditemcharacteristics. Thequalityoftheserepresentationscritically
determinesarecommendersystem’seffectivenessbypreciselycapturingtheinterplaybetweenuserinterestsanditemfeatures.
Recentstudiesemphasizetwofundamentalprinciplesinrepresentationlearning: alignmentanduniformity. Thealignmentprinciple
ensures that embeddings of similar or related items (or users) are closely clustered together, improving the system’s ability to
recommenditemsthatalignwithauser’sinterests. Thisprincipleiscrucialwhenaccuratelyreflectinguserpreferencesthrough
correspondingitemcharacteristics. Conversely,theuniformityprincipleensuresabalanceddistributionofallembeddingsacrossthe
representationspace. Thisapproachpreventstheover-concentrationofembeddingsinspecificareas,enhancingrecommendation
diversityandimprovinggeneralizationtounseendata.
Inthiswork,wefocusonaligningtherepresentationsofpopularandunpopularitemsinteractedwithbythesameuserandre-
weightinguniformitytomitigaterepresentationseparation.OurmodelPAACuniquelyaddressespopularitybiasbycombininggroup
alignmentandcontrastivelearning,afirstinthefield. Unlikepreviousworksthatalignpositiveuser-itempairsorcontrastivepairs,
PAACdirectlyalignspopularandunpopularitems,leveragingtherichinformationofpopularitemstoenhancetherepresentations
ofunpopularitemsandreduceoverfitting. Additionally,weintroducetargetedre-weightingfromapopularity-centricperspectiveto
achieveamorebalancedrepresentation.
5 Conclusion
Inthisstudy,wehaveexaminedpopularitybiasandputforwardPAACasamethodtolessenitsimpact. Wepostulatedthatitems
engagedwithbythesameuserexhibitcommontraits,andweutilizedthisinsighttocoordinatetherepresentationsofbothpopular
andunpopularitemsviaapopularity-conscioussupervisedalignmentmethod. Thisstrategyfurnishedadditionalsupervisorydatafor
lesspopularitems. Itisimportanttonotethatourconceptofaligningandcategorizingitemsaccordingtouser-specificpreferences
introducesafreshperspectiveonalignment. Moreover,wetackledtheproblemofrepresentationseparationseenincurrentCL-based
7modelsbyincorporatingtwohyperparameterstoregulatetheinfluenceofitemswithvaryingpopularitylevelswhenconsidered
aspositiveandnegativesamples. Thismethodrefinedtheuniformityofrepresentationsandsuccessfullyreducedseparation. We
validatedourmethod,PAAC,onthreepubliclyavailabledatasets,demonstratingitseffectivenessandunderlyingrationale.
Inthefuture,wewillexploredeeperalignmentandcontrastadjustmentstailoredtospecifictaskstofurthermitigatepopularity
bias. We aim to investigate the synergies between alignment and contrast and extend our approach to address other biases in
recommendationsystems.
Acknowledgments
ThisworkwassupportedinpartbygrantsfromtheNationalKeyResearchandDevelopmentProgramofChina,theNationalNatural
ScienceFoundationofChina,theFundamentalResearchFundsfortheCentralUniversities,andQuanChengLaboratory.
8